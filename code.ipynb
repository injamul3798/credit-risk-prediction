{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6453754,"sourceType":"datasetVersion","datasetId":3725579}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Description:","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline as pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score, make_scorer\n\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:29.333966Z","iopub.execute_input":"2025-12-07T14:03:29.334323Z","iopub.status.idle":"2025-12-07T14:03:31.174703Z","shell.execute_reply.started":"2025-12-07T14:03:29.334284Z","shell.execute_reply":"2025-12-07T14:03:31.173067Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Define the file path for the input dataset.\nfile_location = '/kaggle/input/loan-default/Loan_default.csv'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:31.175734Z","iopub.execute_input":"2025-12-07T14:03:31.176318Z","iopub.status.idle":"2025-12-07T14:03:31.181637Z","shell.execute_reply.started":"2025-12-07T14:03:31.176285Z","shell.execute_reply":"2025-12-07T14:03:31.179582Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# Load the CSV data directly into a pandas DataFrame.\nData = pd.read_csv(file_location)\n\n# Preview the first few rows to confirm successful loading.\nData.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:31.183148Z","iopub.execute_input":"2025-12-07T14:03:31.183558Z","iopub.status.idle":"2025-12-07T14:03:32.139309Z","shell.execute_reply.started":"2025-12-07T14:03:31.183515Z","shell.execute_reply":"2025-12-07T14:03:32.138201Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       LoanID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  \\\n0  I38PQUQS96   56   85994       50587          520              80   \n1  HPSK72WA7R   69   50432      124440          458              15   \n2  C1OZ6DPJ8Y   46   84208      129188          451              26   \n3  V2KKSFM3UN   32   31713       44799          743               0   \n4  EY08JDHTZP   60   20437        9139          633               8   \n\n   NumCreditLines  InterestRate  LoanTerm  DTIRatio    Education  \\\n0               4         15.23        36      0.44   Bachelor's   \n1               1          4.81        60      0.68     Master's   \n2               3         21.17        24      0.31     Master's   \n3               3          7.07        24      0.23  High School   \n4               4          6.51        48      0.73   Bachelor's   \n\n  EmploymentType MaritalStatus HasMortgage HasDependents LoanPurpose  \\\n0      Full-time      Divorced         Yes           Yes       Other   \n1      Full-time       Married          No            No       Other   \n2     Unemployed      Divorced         Yes           Yes        Auto   \n3      Full-time       Married          No            No    Business   \n4     Unemployed      Divorced          No           Yes        Auto   \n\n  HasCoSigner  Default  \n0         Yes        0  \n1         Yes        0  \n2          No        1  \n3          No        0  \n4          No        0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanID</th>\n      <th>Age</th>\n      <th>Income</th>\n      <th>LoanAmount</th>\n      <th>CreditScore</th>\n      <th>MonthsEmployed</th>\n      <th>NumCreditLines</th>\n      <th>InterestRate</th>\n      <th>LoanTerm</th>\n      <th>DTIRatio</th>\n      <th>Education</th>\n      <th>EmploymentType</th>\n      <th>MaritalStatus</th>\n      <th>HasMortgage</th>\n      <th>HasDependents</th>\n      <th>LoanPurpose</th>\n      <th>HasCoSigner</th>\n      <th>Default</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I38PQUQS96</td>\n      <td>56</td>\n      <td>85994</td>\n      <td>50587</td>\n      <td>520</td>\n      <td>80</td>\n      <td>4</td>\n      <td>15.23</td>\n      <td>36</td>\n      <td>0.44</td>\n      <td>Bachelor's</td>\n      <td>Full-time</td>\n      <td>Divorced</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Other</td>\n      <td>Yes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HPSK72WA7R</td>\n      <td>69</td>\n      <td>50432</td>\n      <td>124440</td>\n      <td>458</td>\n      <td>15</td>\n      <td>1</td>\n      <td>4.81</td>\n      <td>60</td>\n      <td>0.68</td>\n      <td>Master's</td>\n      <td>Full-time</td>\n      <td>Married</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Other</td>\n      <td>Yes</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>C1OZ6DPJ8Y</td>\n      <td>46</td>\n      <td>84208</td>\n      <td>129188</td>\n      <td>451</td>\n      <td>26</td>\n      <td>3</td>\n      <td>21.17</td>\n      <td>24</td>\n      <td>0.31</td>\n      <td>Master's</td>\n      <td>Unemployed</td>\n      <td>Divorced</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>Auto</td>\n      <td>No</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>V2KKSFM3UN</td>\n      <td>32</td>\n      <td>31713</td>\n      <td>44799</td>\n      <td>743</td>\n      <td>0</td>\n      <td>3</td>\n      <td>7.07</td>\n      <td>24</td>\n      <td>0.23</td>\n      <td>High School</td>\n      <td>Full-time</td>\n      <td>Married</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Business</td>\n      <td>No</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EY08JDHTZP</td>\n      <td>60</td>\n      <td>20437</td>\n      <td>9139</td>\n      <td>633</td>\n      <td>8</td>\n      <td>4</td>\n      <td>6.51</td>\n      <td>48</td>\n      <td>0.73</td>\n      <td>Bachelor's</td>\n      <td>Unemployed</td>\n      <td>Divorced</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>Auto</td>\n      <td>No</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\n\n# Drop the 'LoanID' column as it is an identifier and not useful for prediction.\nData = Data.drop(columns='LoanID')\n\n# Verify that the column has been dropped by checking the DataFrame shape.\nprint(\"Updated Data Shape:\", Data.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.141603Z","iopub.execute_input":"2025-12-07T14:03:32.142125Z","iopub.status.idle":"2025-12-07T14:03:32.173694Z","shell.execute_reply.started":"2025-12-07T14:03:32.142080Z","shell.execute_reply":"2025-12-07T14:03:32.172531Z"}},"outputs":[{"name":"stdout","text":"Updated Data Shape: (255347, 17)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"In this step, we drop the 'LoanID' column from the dataset. Often, certain columns are identifiers that donâ€™t provide useful information for modeling, and dropping them can reduce noise and improve model performance.","metadata":{}},{"cell_type":"code","source":"\n\n# Select numerical columns (int and float types) for further analysis and model training.\nnum = Data.select_dtypes(include=['int', 'float'])\n\n# Display the selected numerical features to verify.\nprint(\"Selected Numerical Features:\", num.columns.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.175643Z","iopub.execute_input":"2025-12-07T14:03:32.176099Z","iopub.status.idle":"2025-12-07T14:03:32.188292Z","shell.execute_reply.started":"2025-12-07T14:03:32.176060Z","shell.execute_reply":"2025-12-07T14:03:32.187140Z"}},"outputs":[{"name":"stdout","text":"Selected Numerical Features: ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio', 'Default']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\ndef ColumnTrans(cat):\n    \"\"\"\n    Function to convert categorical variables into numerical variables\n    by mapping unique values to integer indices.\n\n    Parameters:\n    ----------\n    cat : DataFrame\n        A pandas DataFrame containing categorical columns to be transformed.\n\n    Returns:\n    -------\n    cat : DataFrame\n        The original DataFrame with categorical columns transformed into numerical values.\n    \"\"\"\n    # Iterate over each column in the DataFrame.\n    for column in cat.columns:\n        # Get unique values for the column\n        unique_values = cat[column].unique()\n        \n        # Create a mapping of each unique value to a corresponding integer\n        value_map = {value: index for index, value in enumerate(unique_values)}\n        \n        # Map the column's categorical values to their integer indices\n        cat[column] = cat[column].map(value_map)\n    \n    return cat\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.189881Z","iopub.execute_input":"2025-12-07T14:03:32.190321Z","iopub.status.idle":"2025-12-07T14:03:32.204818Z","shell.execute_reply.started":"2025-12-07T14:03:32.190279Z","shell.execute_reply":"2025-12-07T14:03:32.203564Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n\n# Select the categorical columns (object type) from the DataFrame.\ncat = Data.select_dtypes(include='object')\n\n# Apply the custom ColumnTrans function to transform categorical columns into numerical format.\ncat = ColumnTrans(cat)\n\n# Verify the transformation by displaying the first few rows of the transformed categorical data.\nprint(\"Transformed Categorical Columns (first few rows):\")\nprint(cat.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.206193Z","iopub.execute_input":"2025-12-07T14:03:32.206628Z","iopub.status.idle":"2025-12-07T14:03:32.478313Z","shell.execute_reply.started":"2025-12-07T14:03:32.206585Z","shell.execute_reply":"2025-12-07T14:03:32.477019Z"}},"outputs":[{"name":"stdout","text":"Transformed Categorical Columns (first few rows):\n   Education  EmploymentType  MaritalStatus  HasMortgage  HasDependents  \\\n0          0               0              0            0              0   \n1          1               0              1            1              1   \n2          1               1              0            0              0   \n3          2               0              1            1              1   \n4          0               1              0            1              0   \n\n   LoanPurpose  HasCoSigner  \n0            0            0  \n1            0            0  \n2            1            1  \n3            2            1  \n4            1            1  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n\n# Concatenate the numerical and transformed categorical features along the columns (axis=1).\ndf = pd.concat([num, cat], axis=1)\n\n# Verify the combined DataFrame by displaying the first few rows.\nprint(\"Combined DataFrame (first few rows):\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.479229Z","iopub.execute_input":"2025-12-07T14:03:32.479605Z","iopub.status.idle":"2025-12-07T14:03:32.526889Z","shell.execute_reply.started":"2025-12-07T14:03:32.479568Z","shell.execute_reply":"2025-12-07T14:03:32.525582Z"}},"outputs":[{"name":"stdout","text":"Combined DataFrame (first few rows):\n   Age  Income  LoanAmount  CreditScore  MonthsEmployed  NumCreditLines  \\\n0   56   85994       50587          520              80               4   \n1   69   50432      124440          458              15               1   \n2   46   84208      129188          451              26               3   \n3   32   31713       44799          743               0               3   \n4   60   20437        9139          633               8               4   \n\n   InterestRate  LoanTerm  DTIRatio  Default  Education  EmploymentType  \\\n0         15.23        36      0.44        0          0               0   \n1          4.81        60      0.68        0          1               0   \n2         21.17        24      0.31        1          1               1   \n3          7.07        24      0.23        0          2               0   \n4          6.51        48      0.73        0          0               1   \n\n   MaritalStatus  HasMortgage  HasDependents  LoanPurpose  HasCoSigner  \n0              0            0              0            0            0  \n1              1            1              1            0            0  \n2              0            0              0            1            1  \n3              1            1              1            2            1  \n4              0            1              0            1            1  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\n\n# Separate features (X) from the target variable (y)\nx1 = df.drop(columns='Default')  # Features: All columns except 'Default'\ny1 = df['Default']  # Target: 'Default' column\n\n# Verify the separation by displaying the shapes of the features and target.\nprint(\"Shape of Features (X):\", x1.shape)\nprint(\"Shape of Target (y):\", y1.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.528236Z","iopub.execute_input":"2025-12-07T14:03:32.528630Z","iopub.status.idle":"2025-12-07T14:03:32.549908Z","shell.execute_reply.started":"2025-12-07T14:03:32.528589Z","shell.execute_reply":"2025-12-07T14:03:32.546960Z"}},"outputs":[{"name":"stdout","text":"Shape of Features (X): (255347, 16)\nShape of Target (y): (255347,)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n\n# Initialize the resampling techniques\nros = RandomOverSampler()  # Random Over-Sampling to balance the dataset by increasing the minority class.\nrus = RandomUnderSampler()  # Random Under-Sampling to balance the dataset by decreasing the majority class.\nsmote = SMOTE()  # SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic examples for the minority class.\n\n# Verify that the resampling methods are correctly initialized\nprint(\"Resampling techniques initialized:\")\nprint(\"RandomOverSampler:\", ros)\nprint(\"RandomUnderSampler:\", rus)\nprint(\"SMOTE:\", smote)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.551103Z","iopub.execute_input":"2025-12-07T14:03:32.551729Z","iopub.status.idle":"2025-12-07T14:03:32.565450Z","shell.execute_reply.started":"2025-12-07T14:03:32.551696Z","shell.execute_reply":"2025-12-07T14:03:32.564089Z"}},"outputs":[{"name":"stdout","text":"Resampling techniques initialized:\nRandomOverSampler: RandomOverSampler()\nRandomUnderSampler: RandomUnderSampler()\nSMOTE: SMOTE()\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n\n# Apply Random Over-Sampling to increase the minority class\nx2, y2 = ros.fit_resample(x1, y1)\nprint(\"Shape after RandomOverSampler (ROS):\", x2.shape, y2.shape)\n\n# Apply SMOTE to generate synthetic samples for the minority class\nx3, y3 = smote.fit_resample(x2, y2)\nprint(\"Shape after SMOTE:\", x3.shape, y3.shape)\n\n# Apply Random Under-Sampling to decrease the majority class\nx, y = rus.fit_resample(x3, y3)\nprint(\"Shape after RandomUnderSampler (RUS):\", x.shape, y.shape)\n\n# Final balanced dataset\nprint(\"Final Balanced Dataset Shape (X):\", x.shape)\nprint(\"Final Balanced Dataset Shape (Y):\", y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:32.566733Z","iopub.execute_input":"2025-12-07T14:03:32.567225Z","iopub.status.idle":"2025-12-07T14:03:33.387070Z","shell.execute_reply.started":"2025-12-07T14:03:32.567185Z","shell.execute_reply":"2025-12-07T14:03:33.385829Z"}},"outputs":[{"name":"stdout","text":"Shape after RandomOverSampler (ROS): (451388, 16) (451388,)\nShape after SMOTE: (451388, 16) (451388,)\nShape after RandomUnderSampler (RUS): (451388, 16) (451388,)\nFinal Balanced Dataset Shape (X): (451388, 16)\nFinal Balanced Dataset Shape (Y): (451388,)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\n\n# Split the balanced dataset into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Display the shapes of the resulting datasets to verify the split\nprint(\"Shape of Training Features (X_train):\", x_train.shape)\nprint(\"Shape of Testing Features (X_test):\", x_test.shape)\nprint(\"Shape of Training Target (y_train):\", y_train.shape)\nprint(\"Shape of Testing Target (y_test):\", y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:33.388169Z","iopub.execute_input":"2025-12-07T14:03:33.388546Z","iopub.status.idle":"2025-12-07T14:03:33.555475Z","shell.execute_reply.started":"2025-12-07T14:03:33.388511Z","shell.execute_reply":"2025-12-07T14:03:33.553909Z"}},"outputs":[{"name":"stdout","text":"Shape of Training Features (X_train): (361110, 16)\nShape of Testing Features (X_test): (90278, 16)\nShape of Training Target (y_train): (361110,)\nShape of Testing Target (y_test): (90278,)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n\n# Initialize the Random Forest Classifier with 2000 estimators (trees)\nmodel = RandomForestClassifier(n_estimators=2000, random_state=42)\n\n# Display the model's parameters to verify the configuration\nprint(\"Random Forest Classifier initialized with parameters:\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:33.558782Z","iopub.execute_input":"2025-12-07T14:03:33.559146Z","iopub.status.idle":"2025-12-07T14:03:33.565457Z","shell.execute_reply.started":"2025-12-07T14:03:33.559114Z","shell.execute_reply":"2025-12-07T14:03:33.563562Z"}},"outputs":[{"name":"stdout","text":"Random Forest Classifier initialized with parameters:\nRandomForestClassifier(n_estimators=2000, random_state=42)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n# Train the model using the training data\nmodel.fit(x_train, y_train)\n\n# Display a message to indicate that training is complete\nprint(\"Model training complete with Random Forest Classifier.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-07T14:03:33.567245Z","iopub.execute_input":"2025-12-07T14:03:33.567652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Use the trained model to make predictions on the test data\nprediction = model.predict(x_test)\n\n# Display the shape of the predictions to verify\nprint(\"Shape of predictions:\", prediction.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Since the data was imbalanced looking to see if the model predicts both values or needs more work.\n(prediction == 0).sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(prediction == 1).sum()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Import the evaluation metrics from sklearn\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Calculate accuracy, precision, recall, and F1 score\naccuracy = accuracy_score(y_test, prediction)\nprecision = precision_score(y_test, prediction)\nrecall = recall_score(y_test, prediction)\nf1 = f1_score(y_test, prediction)\n\n# Display the metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import XGBoost\nimport xgboost as xgb\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Initialize XGBoost Classifier\nxgb_model = xgb.XGBClassifier(\n    n_estimators=20,\n    learning_rate=0.01,\n    max_depth=6,\n    random_state=42,\n    eval_metric='logloss'\n)\n\n# Train the XGBoost model\nprint(\"Training XGBoost model...\")\nxgb_model.fit(x_train, y_train)\nprint(\"XGBoost model training complete.\")\n\n# Make predictions\nxgb_prediction = xgb_model.predict(x_test)\n\n# Check prediction distribution\nprint(f\"Predictions for class 0: {(xgb_prediction == 0).sum()}\")\nprint(f\"Predictions for class 1: {(xgb_prediction == 1).sum()}\")\n\n# Calculate evaluation metrics\nxgb_accuracy = accuracy_score(y_test, xgb_prediction)\nxgb_precision = precision_score(y_test, xgb_prediction)\nxgb_recall = recall_score(y_test, xgb_prediction)\nxgb_f1 = f1_score(y_test, xgb_prediction)\n\n# Display XGBoost metrics\nprint(\"\\n=== XGBoost Model Performance ===\")\nprint(f\"Accuracy: {xgb_accuracy:.4f}\")\nprint(f\"Precision: {xgb_precision:.4f}\")\nprint(f\"Recall: {xgb_recall:.4f}\")\nprint(f\"F1 Score: {xgb_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import LightGBM\nimport lightgbm as lgb\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Initialize LightGBM Classifier\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=2000,\n    learning_rate=0.01,\n    max_depth=6,\n    random_state=42,\n    verbose=-1\n)\n\n# Train the LightGBM model\nprint(\"Training LightGBM model...\")\nlgb_model.fit(x_train, y_train)\nprint(\"LightGBM model training complete.\")\n\n# Make predictions\nlgb_prediction = lgb_model.predict(x_test)\n\n# Check prediction distribution\nprint(f\"Predictions for class 0: {(lgb_prediction == 0).sum()}\")\nprint(f\"Predictions for class 1: {(lgb_prediction == 1).sum()}\")\n\n# Calculate evaluation metrics\nlgb_accuracy = accuracy_score(y_test, lgb_prediction)\nlgb_precision = precision_score(y_test, lgb_prediction)\nlgb_recall = recall_score(y_test, lgb_prediction)\nlgb_f1 = f1_score(y_test, lgb_prediction)\n\n# Display LightGBM metrics\nprint(\"\\n=== LightGBM Model Performance ===\")\nprint(f\"Accuracy: {lgb_accuracy:.4f}\")\nprint(f\"Precision: {lgb_precision:.4f}\")\nprint(f\"Recall: {lgb_recall:.4f}\")\nprint(f\"F1 Score: {lgb_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
